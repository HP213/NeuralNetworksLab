{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(input):\n",
    "    return 1/(1 + np.exp(-input))\n",
    "\n",
    "#relu activation\n",
    "def relu(input):\n",
    "    return np.maximum(input, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#derivate of a sigmoid w.r.t. input\n",
    "def d_sigmoid(d_init, out):\n",
    "    sig = sigmoid(out)\n",
    "    return d_init * sig * (1 - sig)\n",
    "\n",
    "#derivate of a relu w.r.t. input\n",
    "def d_relu(d_init, out):\n",
    "    d = np.array(d_init, copy = True)\n",
    "    d[out <= 0] = 0.\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init(layers=[4, 5, 1]):\n",
    "    np.random.seed(42)\n",
    "\n",
    "    params_w = {}\n",
    "    params_b = {}\n",
    "\n",
    "    for index in range(len(layers)-1):\n",
    "\n",
    "        layer_num = index + 1\n",
    "        in_layer_size = layers[index]\n",
    "        out_layer_size = layers[index + 1]\n",
    "\n",
    "        params_w['weight' + str(layer_num)] = np.random.randn(out_layer_size, in_layer_size) * 0.1\n",
    "        params_b['bias' + str(layer_num)] = np.random.randn(out_layer_size, 1) * 0.1\n",
    "\n",
    "    return params_w, params_b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_layer_forward_pass(input_activations, weights, bias, activation='R'):\n",
    "    output = np.dot(weights, input_activations) + bias\n",
    "\n",
    "    if activation is 'R':\n",
    "        activation_next = relu(output)\n",
    "    elif activation is 'S':\n",
    "        activation_next = sigmoid(output)\n",
    "    else:\n",
    "        raise Exception('Nahh!')\n",
    "\n",
    "    return activation_next, output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(train_X, params_w, params_b, layers=[4, 5, 1], activate=['R', 'S']):\n",
    "\n",
    "    num_layers = len(layers) - 1\n",
    "\n",
    "    activation_dict = {}\n",
    "    output_dict = {}\n",
    "\n",
    "    curr_act = train_X\n",
    "\n",
    "    for index in range(num_layers):\n",
    "\n",
    "        layer_index = index + 1\n",
    "        prev_act = curr_act      \n",
    "\n",
    "        curr_weight = params_w[\"weight\" + str(layer_index)]\n",
    "        curr_bias = params_b[\"bias\" + str(layer_index)]\n",
    "\n",
    "        curr_act, curr_out = one_layer_forward_pass(prev_act, curr_weight, curr_bias, activate[index])\n",
    "\n",
    "        activation_dict[\"act\" + str(index)] = prev_act\n",
    "        output_dict[\"out\" + str(layer_index)] = curr_out\n",
    "\n",
    "    return curr_act, activation_dict, output_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#binary negative log likelihood loss\n",
    "def cross_entropy_loss(y_pred, train_Y):\n",
    "    num_samples = y_pred.shape[1]\n",
    "    cost = -1 / num_samples * (np.dot(train_Y, np.log(y_pred).T) + np.dot(1 - train_Y, np.log(1 - y_pred).T))\n",
    "    return np.squeeze(cost)\n",
    "\n",
    "#convert probabilities to class prediction with threshold 0.5\n",
    "def get_class_from_probs(probabilities):\n",
    "    class_ = np.copy(probabilities)\n",
    "    class_[class_ > 0.5] = 1\n",
    "    class_[class_ <= 0.5] = 0\n",
    "    return class_\n",
    "\n",
    "#accuracy of predictions (0 to 1)\n",
    "def accuracy_metric(y_pred, train_Y):\n",
    "    y_pred_class = get_class_from_probs(y_pred)\n",
    "    return (y_pred_class == train_Y).all(axis=0).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_layer_backward_pass(curr_grad, curr_weight, curr_bias, curr_out, prev_act, activation='R'):\n",
    "    \n",
    "    num = prev_act.shape[1]\n",
    "\n",
    "    #find out what we are differentiating\n",
    "    if activation is 'R':\n",
    "        d_act_func = d_relu\n",
    "    elif activation is 'S':\n",
    "        d_act_func = d_sigmoid\n",
    "    else:\n",
    "        raise Exception('Nahh!')\n",
    "\n",
    "    #derivative of activation function\n",
    "    d_curr_out = d_act_func(curr_grad, curr_out)\n",
    "\n",
    "    #derivative of weight matrix\n",
    "    d_curr_weight = np.dot(d_curr_out, prev_act.T) / num #shape = (num_current_layer, num_prev_layer)\n",
    "\n",
    "    #derivative of bias matrix\n",
    "    d_curr_bias = np.sum(d_curr_out, axis=1, keepdims=True) / num\n",
    "\n",
    "    #derivative of input activations from previous layer\n",
    "    d_prev_act = np.dot(curr_weight.T, d_curr_out) #shape = (num_prev_layer, 1)\n",
    "\n",
    "    return d_prev_act, d_curr_weight, d_curr_bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(y_pred, train_Y, activation_dict, output_dict, params_w, params_b, layers=[4, 5, 1], activate=['R', 'S']):\n",
    "\n",
    "    gradients = {}\n",
    "\n",
    "    num_samples = train_Y.shape[0]\n",
    "\n",
    "    train_Y = train_Y.reshape(y_pred.shape)\n",
    "\n",
    "    #derivative of binary cross entropy function w.r.t. predictions\n",
    "    d_prev_act = - (np.divide(train_Y, y_pred) - np.divide(1 - train_Y, 1 - y_pred))\n",
    "\n",
    "    num_layers = len(layers) - 1\n",
    "    layer_num = [x + 1 for x in range(num_layers)]\n",
    "    layer_num.reverse()\n",
    "\n",
    "    activate_ = activate\n",
    "    activate_.reverse()\n",
    "\n",
    "    for index, layer_num in enumerate(layer_num):\n",
    "\n",
    "        activation = activate_[layer_num-1]\n",
    "\n",
    "        d_curr_act = d_prev_act\n",
    "\n",
    "        prev_act = activation_dict['act' + str(layer_num - 1)] #activations are one index behind\n",
    "        curr_out = output_dict['out' + str(layer_num)]\n",
    "\n",
    "        curr_weight = params_w['weight' + str(layer_num)]\n",
    "        curr_bias = params_b['bias' + str(layer_num)]\n",
    "\n",
    "        d_prev_act, d_curr_weight, d_curr_bias = one_layer_backward_pass(d_curr_act, curr_weight, curr_bias, curr_out, prev_act, activation)\n",
    "\n",
    "        gradients[\"d_weight\" + str(layer_num)] = d_curr_weight\n",
    "        gradients[\"d_bias\" + str(layer_num)] = d_curr_bias\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_updates(params_w, params_b, gradients, lr, layers=[4, 5, 1]):\n",
    "\n",
    "    for index in range(len(layers) - 1):\n",
    "        #gradient descent\n",
    "        params_w[\"weight\" + str(index + 1)] -= lr * gradients[\"d_weight\" + str(index + 1)]        \n",
    "        params_b[\"bias\" + str(index + 1)] -= lr * gradients[\"d_bias\" + str(index + 1)]\n",
    "\n",
    "    return params_w, params_b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_X, train_Y, epochs, lr, layers=[4, 5, 1], activate=['R', 'S']):\n",
    "    # initiation of neural netowrk parameters\n",
    "    params_w, params_b = init(layers)\n",
    "\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    \n",
    "    # performing calculations for subsequent iterations\n",
    "    for i in range(epochs):\n",
    "        # step forward\n",
    "        y_pred, activations, outputs = forward_pass(train_X, params_w, params_b, layers, activate)\n",
    "        \n",
    "        # monitor loss and accuracy and keep a record of them.\n",
    "        loss = cross_entropy_loss(y_pred, train_Y)\n",
    "        losses.append(loss)\n",
    "        accuracy = accuracy_metric(y_pred, train_Y)\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "        # back prop to calculate the gradients\n",
    "        gradients = backward_pass(y_pred, train_Y, activations, outputs, params_w, params_b)\n",
    "\n",
    "        # update the weights and biases\n",
    "        params_w, params_b = param_updates(params_w, params_b, gradients, lr)\n",
    "        \n",
    "        print('Loss for epoch {} : {}, accuracy is {}'.format(i+1, loss, accuracy))\n",
    "\n",
    "#     y_pred, activations, outputs = forward_pass(train_X, params_w, params_b, layers, activate)\n",
    "    \n",
    "    return params_w, params_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(val_X, val_Y, layers=[4, 5, 1], activate=['R', 'S']):\n",
    "    # initiation of neural net parameters\n",
    "    params_w, params_b = init(layers)\n",
    "\n",
    "    accuracies = []\n",
    "\n",
    "    # step forward\n",
    "    y_pred, activations, outputs = forward_pass(val_X, params_w, params_b, layers, activate)\n",
    "    \n",
    "    # calculating metrics and saving them in history\n",
    "    accuracy = accuracy_metric(y_pred, val_Y)\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "    print('Accuracy is {}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_split(X, Y, train_percent=0.8):\n",
    "\n",
    "    '''\n",
    "        Function takes in the training data as input and returns\n",
    "        a training validation split based on a given percentage.\n",
    "    '''\n",
    "\n",
    "    num_points = X.shape[0]\n",
    "\n",
    "    train_size = int(num_points * 100 * train_percent // 100)\n",
    "\n",
    "    inds = np.arange(num_points)\n",
    "    np.random.shuffle(inds)\n",
    "\n",
    "    train_inds = inds[:train_size]\n",
    "    val_inds = inds[train_size: ]\n",
    "\n",
    "    train_X = X[train_inds, :]\n",
    "    val_X = X[val_inds, :]\n",
    "\n",
    "    train_Y = Y[train_inds]\n",
    "    val_Y = Y[val_inds]\n",
    "\n",
    "    return train_X, train_Y, val_X, val_Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_txt(fname, num_features=4, num_targets=1, num_points=1372):\n",
    "    \n",
    "    '''\n",
    "        Read data from a text file and generate arrays \n",
    "        ready to be fed into the network as inputs.\n",
    "        Each line in the text file is separated by a\n",
    "        newline, and represents a data point.\n",
    "        Features in a line are separated by blank space \n",
    "        and the last data point is the target.\n",
    "    '''\n",
    "\n",
    "    X = np.empty((num_points, num_features), dtype=float)\n",
    "    Y = np.empty(num_points, dtype=int)\n",
    "\n",
    "    with open(fname) as f:\n",
    "        for index, line in enumerate(f):\n",
    "            line = line.rstrip('\\n')\n",
    "            data = line.split(',')\n",
    "\n",
    "\n",
    "            X[index, :] = np.asarray(data[:-1])\n",
    "            Y[index] = np.asarray(data[num_features])\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for epoch 1 : nan, accuracy is 0.5624430264357339\n",
      "Loss for epoch 2 : nan, accuracy is 0.0\n",
      "Loss for epoch 3 : nan, accuracy is 0.0\n",
      "Loss for epoch 4 : nan, accuracy is 0.0\n",
      "Loss for epoch 5 : nan, accuracy is 0.0\n",
      "Loss for epoch 6 : nan, accuracy is 0.0\n",
      "Loss for epoch 7 : nan, accuracy is 0.0\n",
      "Loss for epoch 8 : nan, accuracy is 0.0\n",
      "Loss for epoch 9 : nan, accuracy is 0.0\n",
      "Loss for epoch 10 : nan, accuracy is 0.0\n",
      "Loss for epoch 11 : nan, accuracy is 0.0\n",
      "Loss for epoch 12 : nan, accuracy is 0.0\n",
      "Loss for epoch 13 : nan, accuracy is 0.0\n",
      "Loss for epoch 14 : nan, accuracy is 0.0\n",
      "Loss for epoch 15 : nan, accuracy is 0.0\n",
      "Loss for epoch 16 : nan, accuracy is 0.0\n",
      "Loss for epoch 17 : nan, accuracy is 0.0\n",
      "Loss for epoch 18 : nan, accuracy is 0.0\n",
      "Loss for epoch 19 : nan, accuracy is 0.0\n",
      "Loss for epoch 20 : nan, accuracy is 0.0\n",
      "Loss for epoch 21 : nan, accuracy is 0.0\n",
      "Loss for epoch 22 : nan, accuracy is 0.0\n",
      "Loss for epoch 23 : nan, accuracy is 0.0\n",
      "Loss for epoch 24 : nan, accuracy is 0.0\n",
      "Loss for epoch 25 : nan, accuracy is 0.0\n",
      "Loss for epoch 26 : nan, accuracy is 0.0\n",
      "Loss for epoch 27 : nan, accuracy is 0.0\n",
      "Loss for epoch 28 : nan, accuracy is 0.0\n",
      "Loss for epoch 29 : nan, accuracy is 0.0\n",
      "Loss for epoch 30 : nan, accuracy is 0.0\n",
      "Loss for epoch 31 : nan, accuracy is 0.0\n",
      "Loss for epoch 32 : nan, accuracy is 0.0\n",
      "Loss for epoch 33 : nan, accuracy is 0.0\n",
      "Loss for epoch 34 : nan, accuracy is 0.0\n",
      "Loss for epoch 35 : nan, accuracy is 0.0\n",
      "Loss for epoch 36 : nan, accuracy is 0.0\n",
      "Loss for epoch 37 : nan, accuracy is 0.0\n",
      "Loss for epoch 38 : nan, accuracy is 0.0\n",
      "Loss for epoch 39 : nan, accuracy is 0.0\n",
      "Loss for epoch 40 : nan, accuracy is 0.0\n",
      "Loss for epoch 41 : nan, accuracy is 0.0\n",
      "Loss for epoch 42 : nan, accuracy is 0.0\n",
      "Loss for epoch 43 : nan, accuracy is 0.0\n",
      "Loss for epoch 44 : nan, accuracy is 0.0\n",
      "Loss for epoch 45 : nan, accuracy is 0.0\n",
      "Loss for epoch 46 : nan, accuracy is 0.0\n",
      "Loss for epoch 47 : nan, accuracy is 0.0\n",
      "Loss for epoch 48 : nan, accuracy is 0.0\n",
      "Loss for epoch 49 : nan, accuracy is 0.0\n",
      "Loss for epoch 50 : nan, accuracy is 0.0\n",
      "Loss for epoch 51 : nan, accuracy is 0.0\n",
      "Loss for epoch 52 : nan, accuracy is 0.0\n",
      "Loss for epoch 53 : nan, accuracy is 0.0\n",
      "Loss for epoch 54 : nan, accuracy is 0.0\n",
      "Loss for epoch 55 : nan, accuracy is 0.0\n",
      "Loss for epoch 56 : nan, accuracy is 0.0\n",
      "Loss for epoch 57 : nan, accuracy is 0.0\n",
      "Loss for epoch 58 : nan, accuracy is 0.0\n",
      "Loss for epoch 59 : nan, accuracy is 0.0\n",
      "Loss for epoch 60 : nan, accuracy is 0.0\n",
      "Loss for epoch 61 : nan, accuracy is 0.0\n",
      "Loss for epoch 62 : nan, accuracy is 0.0\n",
      "Loss for epoch 63 : nan, accuracy is 0.0\n",
      "Loss for epoch 64 : nan, accuracy is 0.0\n",
      "Loss for epoch 65 : nan, accuracy is 0.0\n",
      "Loss for epoch 66 : nan, accuracy is 0.0\n",
      "Loss for epoch 67 : nan, accuracy is 0.0\n",
      "Loss for epoch 68 : nan, accuracy is 0.0\n",
      "Loss for epoch 69 : nan, accuracy is 0.0\n",
      "Loss for epoch 70 : nan, accuracy is 0.0\n",
      "Loss for epoch 71 : nan, accuracy is 0.0\n",
      "Loss for epoch 72 : nan, accuracy is 0.0\n",
      "Loss for epoch 73 : nan, accuracy is 0.0\n",
      "Loss for epoch 74 : nan, accuracy is 0.0\n",
      "Loss for epoch 75 : nan, accuracy is 0.0\n",
      "Loss for epoch 76 : nan, accuracy is 0.0\n",
      "Loss for epoch 77 : nan, accuracy is 0.0\n",
      "Loss for epoch 78 : nan, accuracy is 0.0\n",
      "Loss for epoch 79 : nan, accuracy is 0.0\n",
      "Loss for epoch 80 : nan, accuracy is 0.0\n",
      "Loss for epoch 81 : nan, accuracy is 0.0\n",
      "Loss for epoch 82 : nan, accuracy is 0.0\n",
      "Loss for epoch 83 : nan, accuracy is 0.0\n",
      "Loss for epoch 84 : nan, accuracy is 0.0\n",
      "Loss for epoch 85 : nan, accuracy is 0.0\n",
      "Loss for epoch 86 : nan, accuracy is 0.0\n",
      "Loss for epoch 87 : nan, accuracy is 0.0\n",
      "Loss for epoch 88 : nan, accuracy is 0.0\n",
      "Loss for epoch 89 : nan, accuracy is 0.0\n",
      "Loss for epoch 90 : nan, accuracy is 0.0\n",
      "Loss for epoch 91 : nan, accuracy is 0.0\n",
      "Loss for epoch 92 : nan, accuracy is 0.0\n",
      "Loss for epoch 93 : nan, accuracy is 0.0\n",
      "Loss for epoch 94 : nan, accuracy is 0.0\n",
      "Loss for epoch 95 : nan, accuracy is 0.0\n",
      "Loss for epoch 96 : nan, accuracy is 0.0\n",
      "Loss for epoch 97 : nan, accuracy is 0.0\n",
      "Loss for epoch 98 : nan, accuracy is 0.0\n",
      "Loss for epoch 99 : nan, accuracy is 0.0\n",
      "Loss for epoch 100 : nan, accuracy is 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Harsh Bansal\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: RuntimeWarning: invalid value encountered in greater\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\Harsh Bansal\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: RuntimeWarning: invalid value encountered in less_equal\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Users\\Harsh Bansal\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: RuntimeWarning: invalid value encountered in less_equal\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "lr = 0.1\n",
    "\n",
    "X, Y = parse_txt('data/data.txt')\n",
    "train_X, train_Y, val_X, val_Y = train_val_split(X, Y)\n",
    "\n",
    "params_w, params_b = train(train_X.T, train_Y.T, epochs, lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
